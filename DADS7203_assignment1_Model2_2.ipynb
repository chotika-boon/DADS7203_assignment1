{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chotika-boon/DADS7203_assignment1/blob/main/DADS7203_assignment1_Model2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Prepare VM"
      ],
      "metadata": {
        "id": "H-285_sHVINS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_szKYMGqo8A",
        "outputId": "53961a97-3115-4d64-f981-b0f59a95bf69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Install Library"
      ],
      "metadata": {
        "id": "Qq4jID3xHgvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKm7jIWYqr0g",
        "outputId": "64134d41-1397-4428-bb03-13f1103c79ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY3XWjbPG-iW",
        "outputId": "639e73bc-7bd0-4eec-dbef-95e48ee3eae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzjpqwA4UbNk",
        "outputId": "2024ab47-762a-4d91-a3ad-e9fefeb72ead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pythainlp\n",
            "  Downloading pythainlp-3.1.1-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from pythainlp) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->pythainlp) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->pythainlp) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->pythainlp) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->pythainlp) (2.0.12)\n",
            "Installing collected packages: pythainlp\n",
            "Successfully installed pythainlp-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "zb8MubhYCM17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.corpus.common import thai_stopwords\n",
        "thai_stopwords = list(thai_stopwords())"
      ],
      "metadata": {
        "id": "mEPNdgiUUTLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Load Data"
      ],
      "metadata": {
        "id": "gweOJNL6UV6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/gdrive/MyDrive/Lab_2/Lab_2/dataset/tcas61-2.csv\")\n",
        "df.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "c1TalNJjHPdD",
        "outputId": "8256678d-0f96-4f69-8f08-3767ba25cbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    label                                               text\n",
              "55      0                ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡∏±‡∏Å‡∏ó‡∏µ üò≠üò≠\n",
              "93      1                     ‡∏ï‡∏¥‡∏î‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏Å‡∏ô‡∏∞‡∏Ñ‡∏∞ \n",
              "59      0  ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î ‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏î‡∏î‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏±‡∏á‡∏ß...\n",
              "97      0  ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏≠‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ú‡∏•‡∏à‡∏ô‡∏ü‡∏∏‡πà‡∏á‡∏ã‡∏≤‡∏ô‡∏•‡πà‡∏∞ ‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡πâ‡∏ô‡πÅ‡∏ñ‡∏°‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ...\n",
              "36      1  ‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏π‡∏Å‡∏ä‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÇ‡∏ß‡πâ‡∏¢‡∏¢‡∏¢‡∏¢ ‡∏ï‡∏¥‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡∏î‡∏µ‡πÉ‡∏à‡∏°‡∏≤‡∏Å üíïüíïüíïüíïüíïüòçüòç"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d9e7d13a-872c-4d76-a345-252c0258b4e9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0</td>\n",
              "      <td>‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡∏±‡∏Å‡∏ó‡∏µ üò≠üò≠</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>1</td>\n",
              "      <td>‡∏ï‡∏¥‡∏î‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏Å‡∏ô‡∏∞‡∏Ñ‡∏∞</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0</td>\n",
              "      <td>‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î ‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏î‡∏î‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏±‡∏á‡∏ß...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0</td>\n",
              "      <td>‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏≠‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ú‡∏•‡∏à‡∏ô‡∏ü‡∏∏‡πà‡∏á‡∏ã‡∏≤‡∏ô‡∏•‡πà‡∏∞ ‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡πâ‡∏ô‡πÅ‡∏ñ‡∏°‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1</td>\n",
              "      <td>‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏π‡∏Å‡∏ä‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÇ‡∏ß‡πâ‡∏¢‡∏¢‡∏¢‡∏¢ ‡∏ï‡∏¥‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡∏î‡∏µ‡πÉ‡∏à‡∏°‡∏≤‡∏Å üíïüíïüíïüíïüíïüòçüòç</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9e7d13a-872c-4d76-a345-252c0258b4e9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d9e7d13a-872c-4d76-a345-252c0258b4e9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d9e7d13a-872c-4d76-a345-252c0258b4e9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPpex3JeuWQc",
        "outputId": "51ec1cc2-94ef-4ace-9841-7600ea7d8ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(124, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].value_counts().plot.bar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "DvCKqUvXIZb1",
        "outputId": "695273ce-c54e-4e85-9989-8f3560f78974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Train-Test Split"
      ],
      "metadata": {
        "id": "Oe3LFqxEkGdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, test_text, train_labels, test_labels = train_test_split(df['text'], df['label'], \n",
        "                                                                    random_state=101, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "print('train_text shape =',train_text.shape)\n",
        "print('test_text shape =',test_text.shape)\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "#val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                #random_state=101, \n",
        "                                                                #test_size=0.5, \n",
        "                                                                #stratify=temp_labels)\n",
        "\n",
        "#print('val_text shape =',val_text.shape)\n",
        "#print('test_text shape =',test_text.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctf94DK50wgr",
        "outputId": "bb37dc6c-5e18-4b87-91ad-c17dd04ca998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_text shape = (86,)\n",
            "test_text shape = (38,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "from google.colab import widgets"
      ],
      "metadata": {
        "id": "heufWhXRtXj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained = \"Geotrend/bert-base-th-cased\"#@param [\"monsoon-nlp/bert-base-thai\", \"bert-base-multilingual-uncased\", \"Geotrend/bert-base-th-cased\"] "
      ],
      "metadata": {
        "id": "suprmhshyhQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = pretrained"
      ],
      "metadata": {
        "id": "RXQrNLqNyhT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "bert = AutoModel.from_pretrained(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOwHJD4RzEuG",
        "outputId": "7107cfd9-986c-4f34-daec-1f55bc2dcf8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Geotrend/bert-base-th-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at Geotrend/bert-base-th-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "htJsZk46zE4T",
        "outputId": "5170fa91-22f0-4351-e051-d39ffe6667ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 8"
      ],
      "metadata": {
        "id": "YdEPYjM-0r6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "# Parameters max_length = The maximum length (in number of tokens) for the inputs to the transformer model\n",
        "\n",
        "# tokenize and encode sequences in the train set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "#tokens_val = tokenizer.batch_encode_plus(\n",
        "    #val_text.tolist(),\n",
        "    #max_length = max_seq_len,\n",
        "    #pad_to_max_length=True,\n",
        "    #truncation=True,\n",
        "    #return_token_type_ids=False\n",
        "#)\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBI0K9VizE_X",
        "outputId": "1d02ed6e-7a13-44c5-ee03-fbf5e57466d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Integer Sequences to Tensors"
      ],
      "metadata": {
        "id": "p13YjQ9i4kAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.key = ids\n",
        "#2.query = attention_mask\n",
        "#3.value = label (0,1)\n",
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "#val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "#val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "#val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "metadata": {
        "id": "Qzj4E3VwzFDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create DataLoaders**"
      ],
      "metadata": {
        "id": "PY3AS3aQ5_ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "#val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "#val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "#val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "jkSTW33DzFGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unfreeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "-IFnyIepzFJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Model Architecture**"
      ],
      "metadata": {
        "id": "MEL80kuq7LM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "# Applies a linear transformation to the incoming data:\n",
        "# Parameters in_features ‚Äì size of each input sample and out_features ‚Äì size of each output sample\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)    \n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "nXv8yYR6zFM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "4FmiRVvlzFQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCrsk5XUzFX2",
        "outputId": "865a3d97-6ffb-4980-86a1-3d448517f578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Find Class Weights**"
      ],
      "metadata": {
        "id": "MkbfLcQt9Bxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights for unbalanced datasets\n",
        "class_wts = compute_class_weight('balanced', classes= np.unique(train_labels), y= train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx-k2AA4zFbx",
        "outputId": "77166089-59fb-4a9e-a3dc-d255098738bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.76785714 1.43333333]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "Duf5eOjftTMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine Tune Model**"
      ],
      "metadata": {
        "id": "kLSrD47a-RH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        " \n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "mM_nZe8AtXoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "h9KWZG5ptXsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Start Model Training**"
      ],
      "metadata": {
        "id": "-eRgEoge-v3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJttQKAHtTWL",
        "outputId": "5689a5bd-62cf-40b1-cb74-51a02271b6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.274\n",
            "Validation Loss: 0.767\n",
            "\n",
            " Epoch 2 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.265\n",
            "Validation Loss: 0.624\n",
            "\n",
            " Epoch 3 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.977\n",
            "Validation Loss: 1.146\n",
            "\n",
            " Epoch 4 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.698\n",
            "Validation Loss: 1.010\n",
            "\n",
            " Epoch 5 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.832\n",
            "Validation Loss: 0.500\n",
            "\n",
            " Epoch 6 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.580\n",
            "Validation Loss: 0.504\n",
            "\n",
            " Epoch 7 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.533\n",
            "Validation Loss: 0.478\n",
            "\n",
            " Epoch 8 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.479\n",
            "Validation Loss: 0.585\n",
            "\n",
            " Epoch 9 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.599\n",
            "Validation Loss: 0.596\n",
            "\n",
            " Epoch 10 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.836\n",
            "Validation Loss: 0.367\n",
            "\n",
            " Epoch 11 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.538\n",
            "Validation Loss: 0.375\n",
            "\n",
            " Epoch 12 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.571\n",
            "Validation Loss: 0.349\n",
            "\n",
            " Epoch 13 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.484\n",
            "Validation Loss: 0.337\n",
            "\n",
            " Epoch 14 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.457\n",
            "Validation Loss: 0.321\n",
            "\n",
            " Epoch 15 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.476\n",
            "Validation Loss: 0.312\n",
            "\n",
            " Epoch 16 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.425\n",
            "Validation Loss: 0.389\n",
            "\n",
            " Epoch 17 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.677\n",
            "Validation Loss: 0.291\n",
            "\n",
            " Epoch 18 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.667\n",
            "Validation Loss: 0.275\n",
            "\n",
            " Epoch 19 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.438\n",
            "Validation Loss: 0.269\n",
            "\n",
            " Epoch 20 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.279\n",
            "Validation Loss: 0.335\n",
            "\n",
            " Epoch 21 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.462\n",
            "Validation Loss: 0.296\n",
            "\n",
            " Epoch 22 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.442\n",
            "Validation Loss: 0.215\n",
            "\n",
            " Epoch 23 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.446\n",
            "Validation Loss: 0.230\n",
            "\n",
            " Epoch 24 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.306\n",
            "Validation Loss: 0.210\n",
            "\n",
            " Epoch 25 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.303\n",
            "Validation Loss: 0.189\n",
            "\n",
            " Epoch 26 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.378\n",
            "Validation Loss: 0.183\n",
            "\n",
            " Epoch 27 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.479\n",
            "Validation Loss: 0.179\n",
            "\n",
            " Epoch 28 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.437\n",
            "Validation Loss: 0.204\n",
            "\n",
            " Epoch 29 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.361\n",
            "Validation Loss: 0.291\n",
            "\n",
            " Epoch 30 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.487\n",
            "Validation Loss: 0.192\n",
            "\n",
            " Epoch 31 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.597\n",
            "Validation Loss: 0.174\n",
            "\n",
            " Epoch 32 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.348\n",
            "Validation Loss: 0.161\n",
            "\n",
            " Epoch 33 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.347\n",
            "Validation Loss: 0.161\n",
            "\n",
            " Epoch 34 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.322\n",
            "Validation Loss: 0.167\n",
            "\n",
            " Epoch 35 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.281\n",
            "Validation Loss: 0.275\n",
            "\n",
            " Epoch 36 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.696\n",
            "Validation Loss: 0.163\n",
            "\n",
            " Epoch 37 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.310\n",
            "Validation Loss: 0.140\n",
            "\n",
            " Epoch 38 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.335\n",
            "Validation Loss: 0.208\n",
            "\n",
            " Epoch 39 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.478\n",
            "Validation Loss: 0.207\n",
            "\n",
            " Epoch 40 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.280\n",
            "Validation Loss: 0.140\n",
            "\n",
            " Epoch 41 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.392\n",
            "Validation Loss: 0.150\n",
            "\n",
            " Epoch 42 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.459\n",
            "Validation Loss: 0.172\n",
            "\n",
            " Epoch 43 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.563\n",
            "Validation Loss: 1.054\n",
            "\n",
            " Epoch 44 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.273\n",
            "Validation Loss: 0.245\n",
            "\n",
            " Epoch 45 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.495\n",
            "Validation Loss: 0.474\n",
            "\n",
            " Epoch 46 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.420\n",
            "Validation Loss: 0.180\n",
            "\n",
            " Epoch 47 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.401\n",
            "Validation Loss: 0.199\n",
            "\n",
            " Epoch 48 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.399\n",
            "Validation Loss: 0.174\n",
            "\n",
            " Epoch 49 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.310\n",
            "Validation Loss: 0.139\n",
            "\n",
            " Epoch 50 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.421\n",
            "Validation Loss: 0.338\n",
            "\n",
            " Epoch 51 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.290\n",
            "Validation Loss: 0.211\n",
            "\n",
            " Epoch 52 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.456\n",
            "Validation Loss: 0.148\n",
            "\n",
            " Epoch 53 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.416\n",
            "Validation Loss: 0.181\n",
            "\n",
            " Epoch 54 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.283\n",
            "Validation Loss: 0.206\n",
            "\n",
            " Epoch 55 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.647\n",
            "Validation Loss: 0.190\n",
            "\n",
            " Epoch 56 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.410\n",
            "Validation Loss: 0.308\n",
            "\n",
            " Epoch 57 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.390\n",
            "Validation Loss: 0.207\n",
            "\n",
            " Epoch 58 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.512\n",
            "Validation Loss: 0.121\n",
            "\n",
            " Epoch 59 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.577\n",
            "Validation Loss: 0.467\n",
            "\n",
            " Epoch 60 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.418\n",
            "Validation Loss: 0.175\n",
            "\n",
            " Epoch 61 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.378\n",
            "Validation Loss: 0.193\n",
            "\n",
            " Epoch 62 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.387\n",
            "Validation Loss: 0.194\n",
            "\n",
            " Epoch 63 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.431\n",
            "Validation Loss: 0.139\n",
            "\n",
            " Epoch 64 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.271\n",
            "Validation Loss: 0.121\n",
            "\n",
            " Epoch 65 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.339\n",
            "Validation Loss: 0.113\n",
            "\n",
            " Epoch 66 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.447\n",
            "Validation Loss: 0.178\n",
            "\n",
            " Epoch 67 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.287\n",
            "Validation Loss: 0.108\n",
            "\n",
            " Epoch 68 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.237\n",
            "Validation Loss: 0.567\n",
            "\n",
            " Epoch 69 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.756\n",
            "Validation Loss: 0.303\n",
            "\n",
            " Epoch 70 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.629\n",
            "Validation Loss: 0.453\n",
            "\n",
            " Epoch 71 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.492\n",
            "Validation Loss: 0.121\n",
            "\n",
            " Epoch 72 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.332\n",
            "Validation Loss: 0.222\n",
            "\n",
            " Epoch 73 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.467\n",
            "Validation Loss: 0.127\n",
            "\n",
            " Epoch 74 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.211\n",
            "Validation Loss: 0.178\n",
            "\n",
            " Epoch 75 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.462\n",
            "Validation Loss: 0.290\n",
            "\n",
            " Epoch 76 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.330\n",
            "Validation Loss: 0.216\n",
            "\n",
            " Epoch 77 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.345\n",
            "Validation Loss: 0.139\n",
            "\n",
            " Epoch 78 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.354\n",
            "Validation Loss: 0.177\n",
            "\n",
            " Epoch 79 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.350\n",
            "Validation Loss: 0.103\n",
            "\n",
            " Epoch 80 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.367\n",
            "Validation Loss: 0.177\n",
            "\n",
            " Epoch 81 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.345\n",
            "Validation Loss: 0.143\n",
            "\n",
            " Epoch 82 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.215\n",
            "Validation Loss: 0.122\n",
            "\n",
            " Epoch 83 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.305\n",
            "Validation Loss: 0.102\n",
            "\n",
            " Epoch 84 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.410\n",
            "Validation Loss: 0.290\n",
            "\n",
            " Epoch 85 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.428\n",
            "Validation Loss: 0.121\n",
            "\n",
            " Epoch 86 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.421\n",
            "Validation Loss: 0.128\n",
            "\n",
            " Epoch 87 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.300\n",
            "Validation Loss: 0.262\n",
            "\n",
            " Epoch 88 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.303\n",
            "Validation Loss: 0.156\n",
            "\n",
            " Epoch 89 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.222\n",
            "Validation Loss: 0.131\n",
            "\n",
            " Epoch 90 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.252\n",
            "Validation Loss: 0.093\n",
            "\n",
            " Epoch 91 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.351\n",
            "Validation Loss: 0.082\n",
            "\n",
            " Epoch 92 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.382\n",
            "Validation Loss: 0.089\n",
            "\n",
            " Epoch 93 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.455\n",
            "Validation Loss: 0.084\n",
            "\n",
            " Epoch 94 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.375\n",
            "Validation Loss: 0.077\n",
            "\n",
            " Epoch 95 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.258\n",
            "Validation Loss: 0.080\n",
            "\n",
            " Epoch 96 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.489\n",
            "Validation Loss: 0.119\n",
            "\n",
            " Epoch 97 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.306\n",
            "Validation Loss: 0.097\n",
            "\n",
            " Epoch 98 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.233\n",
            "Validation Loss: 0.168\n",
            "\n",
            " Epoch 99 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.308\n",
            "Validation Loss: 0.105\n",
            "\n",
            " Epoch 100 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.307\n",
            "Validation Loss: 0.095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Load Saved Model**"
      ],
      "metadata": {
        "id": "Ku-56cGKEYLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCwRz7DZ-lRx",
        "outputId": "207c84c3-e574-45d7-9e93-e59fd9f829b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get Predictions for Test Data**"
      ],
      "metadata": {
        "id": "t209R_3Q-0yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions for train data\n",
        "with torch.no_grad():\n",
        "  preds = model(train_seq.to(device), train_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "txFGM49y-lU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(train_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM2yLFOH-lYK",
        "outputId": "5660a971-c89b-428e-bb93-5c1638271559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        56\n",
            "           1       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           1.00        86\n",
            "   macro avg       1.00      1.00      1.00        86\n",
            "weighted avg       1.00      1.00      1.00        86\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(train_y, preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "un-lM4RC-lbF",
        "outputId": "6829dc7c-d0c7-46f6-df3a-4c670f76bf5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "col_0   0   1\n",
              "row_0        \n",
              "0      56   0\n",
              "1       0  30"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5dccb7af-e3f0-4407-b324-44f530b1e934\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5dccb7af-e3f0-4407-b324-44f530b1e934')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5dccb7af-e3f0-4407-b324-44f530b1e934 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5dccb7af-e3f0-4407-b324-44f530b1e934');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "KliA9amY_1Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "id": "VGAHOCpu-lgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "692be1ce-3c23-4f9e-d419-0051a1bf5fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.76      0.83        25\n",
            "           1       0.65      0.85      0.73        13\n",
            "\n",
            "    accuracy                           0.79        38\n",
            "   macro avg       0.78      0.80      0.78        38\n",
            "weighted avg       0.82      0.79      0.79        38\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vHiBTT76_-8X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}